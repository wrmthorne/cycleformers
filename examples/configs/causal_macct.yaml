### ONLY REQUIRED ARGUMENTS ###
output_dir: "./outputs"
###############################

# === Training Arguments === #
use_macct: true
learning_rate: 1e-4

per_device_train_batch_size: 4
gradient_accumulation_steps: 4
per_device_eval_batch_size: 4

eval_strategy: steps
eval_steps: 100
logging_strategy: steps
logging_steps: 1

report_to: "wandb"
run_name: "Qwen2.5-3B-macct"


# === Model Configs === #
model_name_or_path: "Qwen/Qwen2.5-3B"
torch_dtype: bfloat16

# Lora
use_peft: true
lora_r: 32
lora_alpha: 64
use_rslora: true
lora_dropout: 0.05
# lora_target_modules: "all-linear"
lora_task_type: "CAUSAL_LM"

# Optimisation
# attn_implementation: "flash_attention_2"
# use_liger_kernel: true
