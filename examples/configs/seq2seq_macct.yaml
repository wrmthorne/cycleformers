### ONLY REQUIRED ARGUMENTS ###
output_dir: "./outputs"
###############################

# === Training Arguments === #
per_device_train_batch_size: 2
gradient_accumulation_steps: 8
per_device_eval_batch_size: 2
logging_strategy: steps
logging_steps: 1


# === Model Configs === #
model_name_or_path: "google/flan-t5-base"

# Lora
use_peft: true
lora_r: 32
lora_alpha: 64
use_rslora: true
lora_dropout: 0.05
# lora_target_modules: "all-linear"
lora_task_type: "CAUSAL_LM"

# Optimisation
# attn_implementation: "flash_attention_2"
# use_liger_kernel: true