### ONLY REQUIRED ARGUMENTS ###
output_dir: "./outputs"
###############################

# === Training Arguments === #
per_device_train_batch_size: 4
gradient_accumulation_steps: 4
per_device_eval_batch_size: 4
logging_strategy: steps
logging_steps: 1


# === Model Configs === #
model_name_or_path: "meta-llama/Llama-3.2-3B"

# Lora
use_peft: true
lora_r: 32
lora_alpha: 64
use_rslora: true
lora_dropout: 0.05
lora_target_modules: "all-linear"
lora_task_type: "CAUSAL_LM"

# Optimisation
# attn_implementation: "flash_attention_2"
# use_liger_kernel: true