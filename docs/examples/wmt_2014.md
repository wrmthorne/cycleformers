# WMT-14 English-German Translation

This example demonstrates how to use Cycleformers for machine translation using the WMT-14 English-German dataset.

## Basic Example

Here's a minimal example using T5 with MACCT for memory-efficient training:

```python
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
from cycleformers import CycleTrainer, CycleTrainingArguments
from datasets import load_dataset
from peft import LoraConfig

# Load base model and tokenizer
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")
tokenizer = AutoTokenizer.from_pretrained("t5-base")

# Load WMT14 dataset
dataset = load_dataset("wmt14", "de-en")
train_en = dataset["train"].select_columns(["translation"]).map(lambda x: {"text": x["translation"]["en"]})
train_de = dataset["train"].select_columns(["translation"]).map(lambda x: {"text": x["translation"]["de"]})

# Configure LoRA adapters for memory efficiency
peft_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["q", "v"],
    bias="none"
)

# Training arguments
args = CycleTrainingArguments(
    output_dir="wmt14-translation",
    use_macct=True,  # Enable adapter-based training
    per_device_train_batch_size=8,
    num_train_epochs=3,
    learning_rate=2e-4,
    warmup_steps=500,
    save_steps=1000,
    eval_steps=1000
)

# Initialize trainer
trainer = CycleTrainer(
    args=args,
    models=model,
    tokenizers=tokenizer,
    train_dataset_A=train_en,
    train_dataset_B=train_de,
    peft_configs=peft_config
)

# Start training
trainer.train()
```

ðŸš§ Under Construction ðŸš§

Coming soon:
- Data preprocessing and cleaning steps
- Tokenization details and special token handling
- Evaluation setup with BLEU and other metrics
- Advanced configuration options:
  - Gradient accumulation
  - Mixed precision training
  - Multi-GPU setup
- Performance benchmarks and memory usage analysis